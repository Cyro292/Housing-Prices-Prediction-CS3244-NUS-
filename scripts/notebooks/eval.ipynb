{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test these please I am not sure\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    r2_score, \n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    median_absolute_error,\n",
    "    explained_variance_score\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def evaluate_regression(y_true, y_pred, n_features=None):\n",
    "    \"\"\"\n",
    "    Evaluates regression predictions by calculating multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): True target values.\n",
    "        y_pred (array-like): Predicted target values.\n",
    "        n_features (int, optional): Number of features used in the model.\n",
    "            If provided, computes Adjusted R².\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with metrics:\n",
    "            - R2: R-squared score.\n",
    "            - Adjusted_R2: Adjusted R-squared (if n_features provided).\n",
    "            - ExplainedVariance: Explained variance score.\n",
    "            - RMSE: Root Mean Squared Error.\n",
    "            - MAE: Mean Absolute Error.\n",
    "            - MAPE: Mean Absolute Percentage Error (%).\n",
    "            - MedianAE: Median Absolute Error.\n",
    "            - Residuals: y_true - y_pred.\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    median_ae = median_absolute_error(y_true, y_pred)\n",
    "    evs = explained_variance_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate MAPE (with safe division)\n",
    "    y_true_safe = np.where(np.array(y_true)==0, 1, np.array(y_true))\n",
    "    mape = np.mean(np.abs((np.array(y_true) - np.array(y_pred)) / y_true_safe)) * 100\n",
    "    \n",
    "    residuals = np.array(y_true) - np.array(y_pred)\n",
    "    \n",
    "    adjusted_r2 = None\n",
    "    n = len(y_true)\n",
    "    if n_features is not None and n > n_features + 1:\n",
    "        adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)\n",
    "    \n",
    "    metrics = {\n",
    "        \"R2\": r2,\n",
    "        \"Adjusted_R2\": adjusted_r2,\n",
    "        \"ExplainedVariance\": evs,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"MedianAE\": median_ae,\n",
    "        \"Residuals\": residuals\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_diagnostics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Generates diagnostic plots:\n",
    "      - Residuals vs. Predicted values.\n",
    "      - Histogram (with KDE) of residuals.\n",
    "      - Scatter plot of Actual vs. Predicted values.\n",
    "    \"\"\"\n",
    "    residuals = np.array(y_true) - np.array(y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(14, 4))\n",
    "    \n",
    "    # Residuals vs. Predicted\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5, color='purple')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residuals vs. Predicted\")\n",
    "    \n",
    "    # Histogram of Residuals\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(residuals, kde=True, color='orange', bins=30)\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.title(\"Residual Distribution\")\n",
    "    \n",
    "    # Actual vs. Predicted\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5, color='green')\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Actual vs. Predicted\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def cross_val_evaluate(model, X, y, cv=5, n_features=None):\n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation and computes evaluation metrics on each fold.\n",
    "    \n",
    "    Parameters:\n",
    "        model: A scikit-learn-like estimator with fit/predict methods.\n",
    "        X (DataFrame or array-like): Features.\n",
    "        y (Series or array-like): Target values.\n",
    "        cv (int): Number of folds for cross-validation.\n",
    "        n_features (int, optional): Number of features for Adjusted R² computation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mean and standard deviation for each metric across folds.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    metrics_list = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = evaluate_regression(y_test, y_pred, n_features=n_features)\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    summary = {}\n",
    "    keys = metrics_list[0].keys()\n",
    "    for key in keys:\n",
    "        values = np.array([m[key] for m in metrics_list if m[key] is not None])\n",
    "        summary[key] = {\"mean\": np.mean(values), \"std\": np.std(values)}\n",
    "    return summary\n",
    "\n",
    "def log_parameters(params, metrics, file_path=\"model_log.json\"):\n",
    "    \"\"\"\n",
    "    Logs model parameters and evaluation metrics to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "        params (dict): Model parameters/hyperparameters.\n",
    "        metrics (dict): Evaluation metrics.\n",
    "        file_path (str): Path to the log JSON file.\n",
    "    \"\"\"\n",
    "    log_entry = {\n",
    "        \"params\": params,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            log_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        log_data = []\n",
    "    \n",
    "    log_data.append(log_entry)\n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(log_data, f, indent=4)\n",
    "    print(f\"Logged parameters and metrics to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample true and predicted values for testing, DONT FORGET TO CHANGE THIS PLEASE LOOK HERE\n",
    "    y_true = np.array([3, -0.5, 2, 7])\n",
    "    y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "    test_metrics = evaluate_regression(y_true, y_pred, n_features=2)\n",
    "    print(\"Enhanced Evaluation Metrics:\")\n",
    "    for key, value in test_metrics.items():\n",
    "        if key != \"Residuals\":\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"Residuals:\", test_metrics[\"Residuals\"])\n",
    "    \n",
    "    # Plot diagnostics\n",
    "    plot_diagnostics(y_true, y_pred)\n",
    "    \n",
    "    # Dummy cross-validation example using a simple model (e.g., Linear Regression)\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pandas as pd\n",
    "    # Create dummy dataset\n",
    "    X_dummy = pd.DataFrame({\n",
    "        'feature1': np.random.rand(100),\n",
    "        'feature2': np.random.rand(100)\n",
    "    })\n",
    "    y_dummy = X_dummy['feature1'] * 3 + X_dummy['feature2'] * (-2) + np.random.randn(100)*0.1\n",
    "    model = LinearRegression()\n",
    "    cv_results = cross_val_evaluate(model, X_dummy, y_dummy, cv=5, n_features=2)\n",
    "    print(\"Cross-validation results:\")\n",
    "    print(cv_results)\n",
    "    \n",
    "    # Dummy parameter logging\n",
    "    dummy_params = {\"model\": \"LinearRegression\", \"fit_intercept\": True}\n",
    "    dummy_metrics = cv_results\n",
    "    log_parameters(dummy_params, dummy_metrics, file_path=\"dummy_model_log.json\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
