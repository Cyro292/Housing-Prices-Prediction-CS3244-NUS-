{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/amonsisowath/VSC/Uni/Sing2425/CS3244/Assignment1/scripts/lib/../data/Resale Flat Prices (Based on Approval Date), 1990 - 1999.csv...\n",
      "Loading data from /Users/amonsisowath/VSC/Uni/Sing2425/CS3244/Assignment1/scripts/lib/../data/Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv...\n",
      "Loading data from /Users/amonsisowath/VSC/Uni/Sing2425/CS3244/Assignment1/scripts/lib/../data/Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv...\n",
      "Loading data from /Users/amonsisowath/VSC/Uni/Sing2425/CS3244/Assignment1/scripts/lib/../data/Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv...\n",
      "Loading data from /Users/amonsisowath/VSC/Uni/Sing2425/CS3244/Assignment1/scripts/lib/../data/Resale flat prices based on registration date from Jan-2017 onwards.csv...\n",
      "Combined dataset shape: (948962, 11)\n",
      "Features shape: (948962, 10)\n",
      "Target shape: (948962,)\n",
      "Selected features: month, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, lease_commence_date, remaining_lease\n",
      "Found 279 NaN values after processing. Handling them...\n",
      "\n",
      "\n",
      "==================================================\n",
      "PCA Dimensionality Reduction Analysis\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.utils import get_cleaned_normalized_data, load_all_resale_data, get_train_split_data\n",
    "\n",
    "\n",
    "X, y = load_all_resale_data()\n",
    "X, y = get_cleaned_normalized_data(X, y)\n",
    "X_train, X_test, y_train, y_test = get_train_split_data(X, y, 0.2)\n",
    "\n",
    "# Add a section analyzing PCA for dimensionality reduction\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"PCA Dimensionality Reduction Analysis\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different numbers of components\n",
    "n_components_list = [50, 100, 500, 1000, 1500, 2000, 3000]\n",
    "pca_train_r2 = []\n",
    "pca_test_r2 = []\n",
    "pca_cv_scores = []\n",
    "\n",
    "for n in n_components_list:\n",
    "    # Create a pipeline with PCA and Linear Regression\n",
    "    pca_pipe = Pipeline([\n",
    "        ('pca', PCA(n_components=n)),\n",
    "        ('regression', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pca_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_train_pred = pca_pipe.predict(X_train)\n",
    "    y_test_pred = pca_pipe.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    pca_train_r2.append(train_r2)\n",
    "    pca_test_r2.append(test_r2)\n",
    "    \n",
    "    # Calculate cross-validation score\n",
    "    cv_score = cross_val_score(pca_pipe, X_train, y_train, cv=5, scoring='r2').mean()\n",
    "    pca_cv_scores.append(cv_score)\n",
    "    \n",
    "    # Get explained variance\n",
    "    pca = pca_pipe.named_steps['pca']\n",
    "    explained_variance = sum(pca.explained_variance_ratio_) * 100\n",
    "    \n",
    "    print(f\"PCA with {n} components:\")\n",
    "    print(f\"  Explained variance: {explained_variance:.2f}%\")\n",
    "    print(f\"  Training R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  CV R²: {cv_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_components_list, pca_train_r2, 'o-', label='Training R²')\n",
    "plt.plot(n_components_list, pca_test_r2, 'o-', label='Test R²')\n",
    "plt.plot(n_components_list, pca_cv_scores, 'o-', label='CV R²')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Performance with PCA Dimensionality Reduction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of components\n",
    "optimal_idx = np.argmax(pca_test_r2)\n",
    "optimal_n = n_components_list[optimal_idx]\n",
    "\n",
    "print(f\"Optimal number of PCA components: {optimal_n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Values for diffrent number of features:\n",
    "\n",
    "[50, 100, 500, 1000, 1500, 2000, 3000]\n",
    "[0.849174028731681, 0.8508046638967771, 0.8543933925998407, 0.8616923092087314, 0.8638630522567969, 0.8665049922760374]\n",
    "\n",
    "PCA for 50 sufficent. And reduces the feature size by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the 50 pca features\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X)\n",
    "np.save('X_pca.npy', X_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
